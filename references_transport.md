# Références

---

## Ressources de données

1. **Kaggle Dataset**  
   Willianoliveiragibin. (2024). *Transport Move Dataset*.  
   Kaggle. https://www.kaggle.com/datasets/willianoliveiragibin/transport-move  
   [Consulté en janvier 2026]

---

## Frameworks et bibliothèques Python

2. **Scikit-learn Documentation**  
   Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python.  
   *Journal of Machine Learning Research*, 12, 2825-2830.  
   https://scikit-learn.org/  
   [Version 1.x utilisée]

3. **Pandas Library**  
   McKinney, W. (2010). Data Structures for Statistical Computing in Python.  
   *Proceedings of the 9th Python in Science Conference*, 56-61.  
   https://pandas.pydata.org/

4. **NumPy**  
   Harris, C. R., et al. (2020). Array programming with NumPy.  
   *Nature*, 585(7825), 357-362.  
   https://numpy.org/

5. **Matplotlib**  
   Hunter, J. D. (2007). Matplotlib: A 2D graphics environment.  
   *Computing in Science & Engineering*, 9(3), 90-95.  
   https://matplotlib.org/

6. **Seaborn**  
   Waskom, M. L. (2021). seaborn: statistical data visualization.  
   *Journal of Open Source Software*, 6(60), 3021.  
   https://seaborn.pydata.org/

7. **Kagglehub**  
   Kaggle. (2024). *Kagglehub Python Package*.  
   https://github.com/Kaggle/kagglehub

---

## Méthodologies et algorithmes de Machine Learning

8. **Gradient Boosting**  
   Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System.  
   *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 785-794.  
   https://doi.org/10.1145/2939672.2939785

9. **Random Forest**  
   Breiman, L. (2001). Random Forests.  
   *Machine Learning*, 45(1), 5-32.  
   https://doi.org/10.1023/A:1010933404324

10. **Logistic Regression**  
    Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013).  
    *Applied Logistic Regression* (3rd ed.). Wiley.

---

## Techniques de prétraitement et gestion du déséquilibre

11. **SMOTE (Synthetic Minority Over-sampling Technique)**  
    Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002).  
    SMOTE: Synthetic Minority Over-sampling Technique.  
    *Journal of Artificial Intelligence Research*, 16, 321-357.  
    https://doi.org/10.1613/jair.953

12. **KNN Imputation**  
    Troyanskaya, O., et al. (2001). Missing value estimation methods for DNA microarrays.  
    *Bioinformatics*, 17(6), 520-525.

---

## Explicabilité et interprétabilité (XAI)

13. **SHAP (SHapley Additive exPlanations)**  
    Lundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions.  
    *Advances in Neural Information Processing Systems*, 30, 4765-4774.  
    https://github.com/slundberg/shap

14. **LIME (Local Interpretable Model-agnostic Explanations)**  
    Ribeiro, M. T., Singh, S., & Guestrin, C. (2016).  
    "Why Should I Trust You?": Explaining the Predictions of Any Classifier.  
    *Proceedings of the 22nd ACM SIGKDD*, 1135-1144.  
    https://doi.org/10.1145/2939672.2939778

---

## Outils d'intelligence artificielle

15. **Claude AI (Anthropic)**  
    Anthropic. (2024). *Claude 4 - Advanced AI Assistant*.  
    https://www.anthropic.com/claude  
    [Utilisé pour l'assistance méthodologique, la structuration du rapport,  
    la revue de code Python et l'optimisation des analyses statistiques]  
    Version : Claude Sonnet 4.5  
    [Consulté en janvier 2026]

---

## Réglementation et éthique

16. **RGPD (Règlement Général sur la Protection des Données)**  
    Union Européenne. (2016). Règlement (UE) 2016/679 du Parlement européen  
    et du Conseil du 27 avril 2016 relatif à la protection des personnes physiques  
    à l'égard du traitement des données à caractère personnel.  
    *Journal officiel de l'Union européenne*, L 119.

17. **Fairness in Machine Learning**  
    Barocas, S., Hardt, M., & Narayanan, A. (2019).  
    *Fairness and Machine Learning: Limitations and Opportunities*.  
    https://fairmlbook.org/

---

## Méthodologie de validation

18. **Cross-Validation**  
    Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation  
    and model selection. *Proceedings of IJCAI*, 14(2), 1137-1145.

19. **GridSearchCV**  
    Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization.  
    *Journal of Machine Learning Research*, 13, 281-305.

---

## Mobilité urbaine et patterns de déplacement (contexte théorique)

20. **Urban Mobility Patterns**  
    González, M. C., Hidalgo, C. A., & Barabási, A. L. (2008).  
    Understanding individual human mobility patterns.  
    *Nature*, 453(7196), 779-782.  
    https://doi.org/10.1038/nature06958

21. **Transport Data Analytics**  
    Zheng, Y., et al. (2015). Trajectory Data Mining: An Overview.  
    *ACM Transactions on Intelligent Systems and Technology*, 6(3), 1-41.

---

## Environnement de développement

22. **Python Software Foundation**  
    Van Rossum, G., & Drake, F. L. (2009). *Python 3 Reference Manual*.  
    Scotts Valley, CA: CreateSpace.  
    https://www.python.org/  
    [Version 3.x utilisée]

23. **Jupyter Notebook**  
    Kluyver, T., et al. (2016). Jupyter Notebooks—a publishing format for reproducible  
    computational workflows. *Positioning and Power in Academic Publishing*, 87-90.  
    https://jupyter.org/

---

