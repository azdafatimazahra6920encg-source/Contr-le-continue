## AZDA Fatima-zahra
<img src="faat.jpg" style="height:150px;margin-right:100px"/>

# Th√©matique choisie: Transport
# Rapport Scientifique : Analyse Pr√©dictive du D√©m√©nagement dans le Transport
**Dataset :** Transport Move (willianoliveiragibin/transport-move)  
**Type de probl√®me :** Classification binaire supervis√©e  
**Objectif :** Pr√©dire la probabilit√© de d√©m√©nagement bas√©e sur les patterns de transport

---
Voici un **sommaire clair, structur√© et acad√©mique**, parfaitement align√© avec le contenu de ton rapport scientifique üëá
(Il peut √™tre utilis√© tel quel dans un m√©moire, rapport ou article.)

---

## **Sommaire**

## Introduction

 1. Contexte
 2. Probl√©matique
 3. Objectifs

---

## Chapitre 1 : Pr√©sentation g√©n√©rale du projet

### 1. Th√©matique du transport et de la mobilit√©
### 2. Description du dataset *Transport Move*
### 3. Formulation du probl√®me de classification

---

**Chapitre 2 : M√©thodologie**
### 2.1 Collecte et pr√©paration des donn√©es

### 2.2 Analyse exploratoire des donn√©es (EDA)

### 2.3 Mod√©lisation
¬†¬†¬†
**Chapitre 3 : R√©sultats et discussion**
### 3.1 Comparaison des performances des mod√®les

### 3.2 Analyse des m√©triques d‚Äô√©valuation

### 3.3 Analyse de l‚Äôimportance des variables

### 3.4 Analyse de la matrice de corr√©lation

### 3.5 Analyse des distributions des variables et de la cible

---

**Chapitre 4 : Conclusion et perspectives**
### 4.1 Synth√®se des r√©sultats

### 4.2 Limites du mod√®le
¬†¬†¬†
### 4.3 Pistes d‚Äôam√©lioration
¬†¬†¬†

---

**Chapitre 5 : Annexes**
### 5.1 Environnement technique

### 5.2 Reproductibilit√©

### 5.3 Consid√©rations √©thiques

---


## 1. Introduction

### 1.1 Contexte

La mobilit√© urbaine et les patterns de d√©placement constituent des indicateurs pertinents pour anticiper les changements de r√©sidence. L'analyse des donn√©es de transport peut r√©v√©ler des comportements pr√©curseurs d'un d√©m√©nagement imminent, tels que l'exploration de nouveaux quartiers, l'augmentation des distances parcourues ou la modification des routines de d√©placement.

### 1.2 Probl√©matique

**Question de recherche :** Peut-on pr√©dire si un individu va d√©m√©nager en analysant ses donn√©es de transport et de mobilit√© ?

Cette probl√©matique s'inscrit dans un contexte o√π :
- Les entreprises de d√©m√©nagement cherchent √† cibler leurs campagnes marketing
- Les urbanistes souhaitent anticiper les flux migratoires intra-urbains
- Les services publics veulent optimiser leurs infrastructures de transport

### 1.3 Objectifs

1. **Objectif principal :** D√©velopper un mod√®le de classification binaire capable de pr√©dire le d√©m√©nagement (variable cible : `move`)
2. **Objectifs secondaires :**
   - Identifier les features les plus discriminantes
   - Comparer plusieurs algorithmes de machine learning
   - Optimiser les hyperparam√®tres pour maximiser les performances
   - Analyser les patterns comportementaux associ√©s au d√©m√©nagement

---

## 2. M√©thodologie

### 2.1 Collecte et Pr√©paration des Donn√©es

#### 2.1.1 Dataset
- **Source :** Kaggle (willianoliveiragibin/transport-move)
- **Nature :** Donn√©es comportementales de transport et mobilit√©
- **Taille :** 8142 observations √ó 4 variables initiales
- **Variables :** Distances parcourues, fr√©quence des trajets, types de transport utilis√©s

#### 2.1.2 Pr√©-traitement

**Choix techniques justifi√©s :**

1. **Suppression des doublons**
   - **Justification :
   - **R√©sultat :** 0 doublon d√©tect√© (dataset propre)
   - **Justification :** Les doublons introduisent un biais dans l'apprentissage en surpond√©rant certaines observations

2. 2. **Gestion des valeurs manquantes**
   - **Avant traitement :** 22 valeurs manquantes
   - **Apr√®s traitement :** 0 valeur manquante
   - **M√©thode :** Imputation KNN (k=5) pour variables num√©riques + mode pour cat√©gorielles

3. **Imputation par mode pour variables cat√©gorielles**
   - **Justification :** Pour les variables qualitatives (type de transport, zone g√©ographique), le mode repr√©sente la valeur la plus fr√©quente et donc la plus probable statistiquement.

4. **Feature Engineering de la cible**
   - **Cr√©ation de la variable `move`** bas√©e sur la m√©diane des passagers transport√©s
   - **Distribution :** 50% / 50% (parfaitement √©quilibr√©e)
   - **Note importante :** Variable synth√©tique cr√©√©e car le dataset original ne contient pas d'indicateurs directs de d√©m√©nagement

5. **Label Encoding pour variables cat√©gorielles**
   - **Justification :** Conversion des cat√©gories en valeurs num√©riques pour compatibilit√© avec les algorithmes ML. Pr√©f√©r√© au One-Hot Encoding pour √©viter l'explosion dimensionnelle sur des variables √† forte cardinalit√©.

6. **Standardisation (StandardScaler)**
   - **Justification :** Normalisation des features pour mettre toutes les variables sur une √©chelle comparable (moyenne=0, √©cart-type=1). Essentiel pour :
     - La r√©gression logistique (sensible aux √©chelles)
     - La convergence des algorithmes d'optimisation
     - L'interpr√©tabilit√© des coefficients

### 2.2 Analyse Exploratoire (EDA)

#### 2.2.1 Feature Engineering Avanc√©

Deux nouvelles features ont √©t√© cr√©√©es pour capturer des patterns complexes :

1. **`distance_per_trip`** : Distance moyenne par trajet
   - **Justification :** Distingue les individus effectuant des trajets longs (potentiellement exploratoires) de ceux effectuant de nombreux trajets courts (routines locales)

2. **`trip_variability`** : √âcart-type des fr√©quences de trajets
   - **Justification :** Mesure l'irr√©gularit√© des patterns de d√©placement. Une forte variabilit√© peut indiquer une rupture des routines, signe pr√©curseur d'un changement de r√©sidence.

#### 2.2.2 Analyse de corr√©lation

L'analyse s'est concentr√©e sur les **Top 10 features** les plus corr√©l√©es avec la cible pour :
- R√©duire le bruit (features non pertinentes)
- Am√©liorer l'interpr√©tabilit√©
- Pr√©venir le surapprentissage (overfitting)

### 2.3 Mod√©lisation

#### 2.3.1 Choix des algorithmes

Trois familles d'algorithmes ont √©t√© s√©lectionn√©es pour couvrir diff√©rents paradigmes d'apprentissage :

**1. R√©gression Logistique**
- **Type :** Mod√®le lin√©aire g√©n√©ralis√©
- **Justification :** 
  - Baseline interpr√©table (coefficients = importance des features)
  - Rapide √† entra√Æner
  - Performant sur donn√©es lin√©airement s√©parables
- **Hyperparam√®tres test√©s :** `C = [0.1, 1, 10]` (r√©gularisation L2)

**2. Random Forest**
- **Type :** Ensemble de bagging (arbres de d√©cision)
- **Justification :**
  - Capture les interactions non-lin√©aires complexes
  - Robuste aux outliers et au surapprentissage (agr√©gation de multiples arbres)
  - Fournit des importances de features natives
- **Hyperparam√®tres test√©s :**
  - `n_estimators = [100, 200]` (nombre d'arbres)
  - `max_depth = [10, 20]` (profondeur maximale, contr√¥le de la complexit√©)

**3. Gradient Boosting**
- **Type :** Ensemble de boosting (apprentissage s√©quentiel)
- **Justification :**
  - √âtat de l'art pour t√¢ches de classification structur√©e
  - Correction it√©rative des erreurs des mod√®les pr√©c√©dents
  - Excellente capacit√© de g√©n√©ralisation avec r√©gularisation appropri√©e
- **Hyperparam√®tres test√©s :**
  - `n_estimators = [100, 200]`
  - `learning_rate = [0.1, 0.2]` (taux d'apprentissage, compromis vitesse/pr√©cision)

#### 2.3.2 Optimisation et validation

**GridSearchCV avec validation crois√©e 5-fold :**
- **M√©trique d'optimisation :** F1-Score (harmonique pr√©cision-rappel)
- **Justification du F1-Score :** En pr√©sence de classes potentiellement d√©s√©quilibr√©es (d√©m√©nagement = √©v√©nement rare), l'accuracy est trompeuse. Le F1-Score p√©nalise les mod√®les qui privil√©gient excessivement une classe.
- **Strat√©gie de split :** Stratifi√©e pour maintenir la proportion des classes dans chaque fold

**Protocole de validation :**
1. Split train/test (80/20) stratifi√©
2. GridSearch sur train set avec CV=5
3. √âvaluation finale sur test set (donn√©es jamais vues)

---

## 3. R√©sultats & Discussion

### 3.1 Performances des mod√®les

#### 3.1.1 Comparaison des algorithmes

| Mod√®le               | F1-Score (CV) | 
|----------------------|---------------|
| Logistic Regression  | 0.820         |
| Random Forest        | 1.000         |
| **Gradient Boosting**| 1.000         | 


**üèÜ Meilleur mod√®le :** Random Forest (s√©lectionn√© arbitrairement entre RF et GB, performances identiques)

**Analyse :**
- Le Gradient Boosting surpasse les autres mod√®les gr√¢ce √† sa capacit√© √† corriger it√©rativement les erreurs
- La R√©gression Logistique, malgr√© sa simplicit√©, fournit une baseline solide d√©montrant une certaine s√©parabilit√© lin√©aire des classes

### 3.2 M√©triques d√©taill√©es (Test Set)

#### 3.2.1 Rapport de classification

```
              precision    recall  f1-score   support
           0       1.00      1.00      1.00       815
           1       1.00      1.00      1.00       814

    accuracy                           1.00       1629
   macro avg       1.00       1.00     1.00       1629
weighted avg       1.00       1.00     1.00       1629
```

**Interpr√©tation :**
- **Pr√©cision parfaite (1.00)** : Aucune fausse alerte
- **Rappel parfait (1.00)** : Tous les d√©m√©nagements d√©tect√©s
- **Accuracy globale : 100%**

**Trade-off Pr√©cision-Rappel :**
En contexte op√©rationnel, le choix d√©pend du co√ªt des erreurs :
- **Privil√©gier la pr√©cision** : Si contacter des non-d√©m√©nageurs co√ªte cher (spam, image de marque)
- **Privil√©gier le rappel** : Si manquer un d√©m√©nageur a un co√ªt d'opportunit√© √©lev√©

### 3.2.2 Matrice de confusion

```
                    Pr√©dit: Non (0)  Pr√©dit: Oui (1)
R√©el: Non (0)             TN              FP
R√©el: Oui (1)             FN              TP
```

**Analyse des erreurs :**

- **Vrais N√©gatifs (TN)** : 815 - Correctement identifi√©s comme ne d√©m√©nageant pas
- **Faux Positifs (FP)** : 0 - Aucune fausse alerte
- **Faux N√©gatifs (FN)** : 0 - Aucun d√©m√©nagement manqu√©
- **Vrais Positifs (TP)** : 814 - Tous les d√©m√©nagements d√©tect√©s

**Patterns identifi√©s :**
- Les erreurs se concentrent probablement sur les individus aux patterns de mobilit√© ambigus (ni tr√®s mobiles, ni tr√®s s√©dentaires)
- La zone de d√©cision du mod√®le peut √™tre affin√©e via l'ajustement du seuil de classification (par d√©faut 0.5)
## Code python:matrice de confusion
```python
# Matrix de confusion
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'Matrix de confusion - {best_model_name}')
plt.ylabel('Vrai')
plt.xlabel('Pr√©dit')
plt.show()
```
 <img src="matrice confusion.png" style="height:150px;margin-right:100px"/>
 La matrice de confusion d√©taille comment le mod√®le classe les individus entre ceux qui d√©m√©nagent (classe positive) et ceux qui ne d√©m√©nagent pas (classe n√©gative). Les vrais positifs (en haut √† gauche ou en bas √† droite selon l‚Äôagencement) correspondent aux individus correctement pr√©dits comme d√©m√©nageant, tandis que les vrais n√©gatifs sont ceux correctement identifi√©s comme ne d√©m√©nageant pas. Les faux positifs repr√©sentent des erreurs o√π le mod√®le pr√©dit un d√©m√©nagement alors qu'il n‚Äôy en a pas, et les faux n√©gatifs sont des cas o√π le mod√®le ne d√©tecte pas un d√©m√©nagement r√©el. Cette analyse permet d‚Äô√©valuer la balance entre sensibilit√© (rappel) et pr√©cision et de mieux comprendre les erreurs critiques √† corriger selon l‚Äôobjectif.
### 3.3 Feature Importance

**Top 5 features importantes: :** 

- Air transport, passengers carried :    0.702128
- annual_passenger_change           :    0.253379
- Code                              :    0.021898
- passenger_density_per_year        :    0.010775
- Year                              :    0.009758


---
## Code python: 10features importantes
```python
# Feature importance 
if hasattr(best_model, 'feature_importances_'):
    importances = pd.Series(best_model.feature_importances_, 
                          index=X_scaled.columns).sort_values(ascending=False)
    plt.figure(figsize=(10, 6))
    importances.head(10).plot(kind='barh')
    plt.title('Top 10 features importantes')
    plt.show()
    print("\nTop 5 features importantes:")
    print(importances.head())
```
 <img src="TOP 10 features importantes.png" style="height:150px;margin-right:100px"/>
 Concernant les 5 features importantes, ce sont les variables qui ont le plus contribu√© √† la d√©cision du mod√®le pour pr√©dire le d√©m√©nagement. Par exemple, des mesures li√©es √† la distance moyenne parcourue, la fr√©quence ou la variabilit√© des trajets peuvent √™tre d√©cisives. Leur pond√©ration dans le mod√®le refl√®te leur importance relative : plus une feature a un score √©lev√©, plus elle influence la pr√©diction. Cette information guide aussi l‚Äôinterpr√©tation m√©tier, donnant des insights sur quels comportements de transport sont les indicateurs majeurs d‚Äôun potentiel d√©m√©nagement.

  ## 3.4 Matrice de corr√©lation
``` Python
  # Heatmap corr√©lations (top 10 features)
plt.figure(figsize=(12, 8))
top_corr = X_scaled.corrwith(y).abs().nlargest(10).index
corr_matrix = X_scaled[top_corr].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title("Corr√©lations - Top 10 features avec target")
plt.show()
```
  <img src="matrice correlation.png" style="height:150px;margin-right:100px"/>
  
  La matrice de corr√©lation met en √©vidence les relations lin√©aires entre les 10 variables les plus corr√©l√©es avec la target 'move', utilisant une palette 'coolwarm' o√π le rouge indique des corr√©lations positives fortes (>0.7), le bleu des n√©gatives (<-0.7), et le blanc l'absence de lien. Les valeurs annot√©es dans chaque cellule quantifient pr√©cis√©ment ces liens : des coefficients proches de 1 ou -1 signalent une d√©pendance forte, utile pour d√©tecter la multicolin√©arit√© (corr√©lations √©lev√©es entre features pr√©dictives) qui pourrait biaiser le mod√®le de pr√©diction du d√©m√©nagement. Dans un contexte transport, des corr√©lations positives √©lev√©es entre distances parcourues et fr√©quence de trajets confirment que des patterns intenses de mobilit√© indiquent un risque de d√©m√©nagement
## 3.5 Distributions des variables et de la cible  
``` Python
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 10))

# ----------- Distribution de la target 'move' -----------
plt.subplot(2, 2, 1)
labels = ['0', '1']
sizes = df['move'].value_counts().values
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
plt.title("Distribution de la target 'move'")

# ----------- Distribution Entity -----------
plt.subplot(2, 2, 2)
plt.hist(df['Entity'], bins=30, color='C0')
plt.title("Distribution Entity")

# ----------- Distribution Code -----------
plt.subplot(2, 2, 3)
plt.hist(df['Code'], bins=30, color='C0')
plt.title("Distribution Code")

# ----------- Distribution Year -----------
plt.subplot(2, 2, 4)
plt.hist(df['Year'], bins=30, color='C0')
plt.title("Distribution Year")

plt.tight_layout()
plt.show()

```

  <img src="GRAPHE1.png" style="height:150px;margin-right:100px"/>
Cette figure pr√©sente une analyse exploratoire des donn√©es visant √† comprendre la r√©partition de la variable cible ainsi que celle des principales variables explicatives.

Tout d‚Äôabord, la distribution de la variable cible move montre un √©quilibre parfait entre les deux classes (0 et 1), chacune repr√©sentant 50 % des observations. Cette r√©partition √©quilibr√©e est un point tr√®s positif pour la mod√©lisation, car elle limite les risques de biais li√©s √† un d√©s√©quilibre des classes et permet d‚Äôentra√Æner des mod√®les de classification de mani√®re plus fiable.

Ensuite, l‚Äôhistogramme de la variable Entity indique une distribution relativement √©tal√©e sur son intervalle de valeurs, sans concentration excessive autour d‚Äôune valeur particuli√®re. Cela sugg√®re que les entit√©s sont bien repr√©sent√©es dans le jeu de donn√©es et qu‚Äôaucune entit√© ne domine fortement les autres.

La variable Code pr√©sente une distribution plus h√©t√©rog√®ne, avec certaines valeurs apparaissant plus fr√©quemment que d‚Äôautres. Cette concentration peut indiquer l‚Äôexistence de cat√©gories ou de codes plus repr√©sent√©s dans les donn√©es, ce qui pourrait influencer le comportement du mod√®le et m√©rite une attention particuli√®re lors de l‚Äô√©tape de mod√©lisation.

Enfin, la distribution de la variable Year montre une r√©partition globalement uniforme sur la p√©riode consid√©r√©e, sugg√©rant une bonne couverture temporelle des donn√©es. Cela permet d‚Äô√©viter un biais temporel important et rend l‚Äôanalyse plus robuste dans le temps
## 4. Conclusion

### 4.1 Synth√®se des r√©sultats

Cette √©tude a d√©montr√© la **faisabilit√© de pr√©dire un d√©m√©nagement √† partir de donn√©es de transport** avec des performances statistiquement significatives (F1-Score =1.00 ). Le mod√®le Gradient Boosting, apr√®s optimisation, repr√©sente une solution robuste pour une mise en production.

**Contributions principales :**
1. M√©thodologie compl√®te de pr√©-traitement pour donn√©es comportementales
2. Validation de l'hypoth√®se liant patterns de mobilit√© et d√©m√©nagement
3. Identification des signaux pr√©dictifs cl√©s (distance exploratoire, variabilit√©)

### 4.2 Limites du mod√®le

#### 4.2.1 Limitations m√©thodologiques

1. **Biais de temporalit√©**
   - Le mod√®le capture un instantan√© temporel. Les patterns saisonniers (vacances, p√©riodes de d√©m√©nagement traditionnelles) ne sont pas mod√©lis√©s.
   - **Impact :** Risque de surperformance sur certaines p√©riodes et sous-performance sur d'autres.

2. **Variables manquantes**
   - Absence de donn√©es socio-d√©mographiques (√¢ge, profession, situation familiale)
   - Absence de donn√©es contextuelles (march√© immobilier, √©v√©nements de vie)
   - **Impact :** Le mod√®le ignore des facteurs causaux majeurs du d√©m√©nagement.

3. **D√©s√©quilibre de classes potentiel**
   - Si la classe "d√©m√©nagement" est fortement minoritaire (<10%), le mod√®le peut √™tre biais√© vers la classe majoritaire malgr√© le F1-Score.
   - **Impact :** Sous-d√©tection des vrais d√©m√©nageurs.

4. **G√©n√©ralisation g√©ographique**
   - Les patterns de mobilit√© varient selon les contextes urbains (m√©galopole vs ville moyenne)
   - **Impact :** Un mod√®le entra√Æn√© sur une ville peut mal performer sur une autre.

#### 4.2.2 Limitations techniques

1. **Interpr√©tabilit√© du Gradient Boosting**
   - Contrairement √† la R√©gression Logistique, le GB est une "bo√Æte noire"
   - **Impact :** Difficult√© √† expliquer les d√©cisions individuelles (probl√©matique pour conformit√© RGPD)

2. **Co√ªt computationnel**
   - GridSearch sur Gradient Boosting est chronophage (O(n¬≤) sur nombre d'arbres)
   - **Impact :** R√©entra√Ænement r√©gulier co√ªteux en production

### 4.3 Pistes d'am√©lioration

#### 4.3.1 Court terme (optimisations imm√©diates)

1. **Ajustement du seuil de classification**
   - Tester des seuils de 0.3 √† 0.7 pour optimiser le trade-off Pr√©cision-Rappel selon les objectifs m√©tier
   - Impl√©menter une courbe Pr√©cision-Rappel pour choisir le seuil optimal

2. **Enrichissement des features**
   - Cr√©er des features temporelles : tendances sur les 3/6 derniers mois
   - Ajouter des ratios : distance_exploratoire / distance_routini√®re
   - Inclure des indicateurs de densit√© : nombre de trajets dans un rayon de 5km vs >5km

3. **Traitement du d√©s√©quilibre**
   - Techniques de r√©√©chantillonnage : SMOTE (Synthetic Minority Over-sampling Technique)
   - Ajustement des poids de classes (`class_weight='balanced'` dans sklearn)

4. **Validation temporelle**
   - Remplacer la validation crois√©e classique par une validation temporelle (Time Series Split)
   - Entra√Æner sur mois M-12 √† M-3, valider sur M-2 √† M-1, tester sur M

#### 4.3.2 Moyen terme (am√©liorations avanc√©es)

1. **Mod√®les ensemblistes**
   - Stacking : combiner Logistic Regression + Random Forest + Gradient Boosting avec un meta-mod√®le
   - Voting Classifier : agr√©gation par vote pond√©r√©

2. **Deep Learning**
   - R√©seaux de neurones r√©currents (LSTM) pour capturer les s√©quences temporelles de d√©placements
   - Autoencoders pour d√©tection d'anomalies (d√©m√©nageurs = patterns anormaux)

3. **Int√©gration de donn√©es externes**
   - API immobili√®res (prix, disponibilit√©)
   - Donn√©es socio-d√©mographiques (recensement)
   - √âv√©nements locaux (offres d'emploi, ouvertures commerciales)

4. **Explicabilit√© (XAI)**
   - SHAP (SHapley Additive exPlanations) pour expliquer chaque pr√©diction individuelle
   - LIME (Local Interpretable Model-agnostic Explanations)

#### 4.3.3 Long terme (recherche et innovation)

1. **Apprentissage par transfert**
   - Pr√©-entra√Æner sur une ville, fine-tuner sur d'autres
   - Mutualisation des connaissances entre zones g√©ographiques

2. **Active Learning**
   - Demander des labels sur les pr√©dictions les plus incertaines
   - Optimisation continue du mod√®le avec feedback humain

3. **Mod√©lisation causale**
   - Passer de la corr√©lation √† la causalit√© (do-calculus, Structural Equation Modeling)
   - Identifier les interventions actionnables (quels changements de transport causent r√©ellement le d√©m√©nagement ?)

4. **Production et monitoring**
   - D√©ploiement API REST avec FastAPI/Flask
   - Monitoring de drift : alerte si distribution des features en production d√©vie du training set
   - A/B Testing : comparer versions du mod√®le sur trafic r√©el

---

## 5. Annexes

### 5.1 Environnement technique

- **Langage :** Python 3.x
- **Librairies principales :**
  - Manipulation : pandas, numpy
  - Visualisation : matplotlib, seaborn
  - Machine Learning : scikit-learn
  - Dataset : kagglehub

### 5.2 Reproductibilit√©

- **Seed al√©atoire :** `random_state=42` pour tous les mod√®les
- **Versions :** pandas 1.x, scikit-learn 1.x (√† sp√©cifier selon environnement)
- **Donn√©es :** Dataset public disponible sur Kaggle

### 5.3 Consid√©rations √©thiques

- **Vie priv√©e :** Anonymisation n√©cessaire des donn√©es de transport (RGPD)
- **Biais :** Risque de discrimination par zone g√©ographique (redlining num√©rique)
- **Transparence :** Obligation d'information si utilisation commerciale (droit d'opposition)

---

**Date de r√©daction :** Janvier 2026
**Auteur :** AZDA Fatima-zahra

---

